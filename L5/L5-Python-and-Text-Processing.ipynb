{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts in text processing\n",
    "\n",
    "## Corpora (语料库)\n",
    "\n",
    "Corpus is a large collection of texts. It is a body of written or spoken material upon which a linguistic analysis is based. A corpus provides grammarians, lexicographers, and other interested parties with better discriptions of a language. Computer-procesable corpora allow linguists to adopt the principle of total accountability, retrieving all the occurrences of a particular word or structure for inspection or randomly selcted samples. Corpus analysis provide lexical information, morphosyntactic information, semantic information and pragmatic information.\n",
    "\n",
    "\n",
    "\n",
    "## Tokens\n",
    "\n",
    "A token is the technical name for a sequence of characters, that we want to treat as a group. The vocabulary of a text is just the set of tokens that it uses, since in a set, all duplicates are collapsed together. In Python we can obtain the vocabulary items with the command: `set()`.\n",
    "\n",
    "## Stopwords\n",
    "\n",
    "\n",
    "Stopwords are common words that generally do not contribute to the meaning of a sentence,\n",
    "at least for the purposes of information retrieval and natural language processing. These are\n",
    "words such as the and a. Most search engines will filter out stopwords from search queries\n",
    "and documents in order to save space in their index.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Stemming is a technique to remove affixes from a word, ending up with the stem. For\n",
    "example, the stem of cooking is cook , and a good stemming algorithm knows that the ing\n",
    "suffix can be removed. Stemming is most commonly used by search engines for indexing\n",
    "words. Instead of storing all forms of a word, a search engine can store only the stems, greatly\n",
    "reducing the size of index while increasing retrieval accuracy.\n",
    "\n",
    "\n",
    "## Frequency Counts （频数统计）\n",
    "\n",
    "Frequency Counts the number of hits. Frequency counts require finding all the occurences of a particular feature in the corpus. So it is implicit in concordancing. Software is used for this purpose. Frequency counts can be explained statistically. \n",
    "\n",
    "## Word Segmenter （分词）\n",
    "\n",
    "Word segmentation is the problem of dividing a string of written language into its component words.\n",
    "\n",
    "In English and many other languages using some form of the Latin alphabet, the space is a good approximation of a word divider (word delimiter). (Some examples where the space character alone may not be sufficient include contractions like can't for can not.)\n",
    "\n",
    "However the equivalent to this character is not found in all written scripts, and without it word segmentation is a difficult problem. Languages which do not have a trivial word segmentation process include Chinese, Japanese, where sentences but not words are delimited, Thai and Lao, where phrases and sentences but not words are delimited, and Vietnamese, where syllables but not words are delimited.\n",
    "\n",
    "\n",
    "## Part-Of-Speech Tagger （词性标注工具）\n",
    "\n",
    "In corpus linguistics, part-of-speech tagging (POS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition, as well as its context—i.e. relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "\n",
    "## Named Entity Recognizer（命名实体识别工具）\n",
    "\n",
    "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify elements in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages.\n",
    "\n",
    "## Parser（句法分析器）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will treat text as `raw data` for the programs we write, programs that manipulate and analyze it in\n",
    "a variety of interesting ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing tools\n",
    "\n",
    "- Natural Language Toolkit\n",
    "\n",
    "    **NLTK** is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "    \n",
    "    Natural Language Processing with Python provides a practical introduction to programming for language processing. Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more. The book is being updated for Python 3 and NLTK 3.\n",
    "    \n",
    "    \n",
    "- Stanford Word Segmenter\n",
    "\n",
    "    Tokenization of raw text is a standard pre-processing step for many NLP tasks. For English, tokenization usually involves punctuation splitting and separation of some affixes like possessives. Other languages require more extensive token pre-processing, which is usually called segmentation.\n",
    "\n",
    "    The Stanford Word Segmenter currently supports Arabic and Chinese. The provided segmentation schemes have been found to work well for a variety of applications. \n",
    "    \n",
    "- NLP toolkits for Chinese\n",
    "\n",
    "    - [Toolkit for Chinese natural language processing](https://github.com/xpqiu/fnlp/)\n",
    "    \n",
    "    - [The ICT Natural Language Processing Research Group](http://nlp.ict.ac.cn/english/)\n",
    "    \n",
    "    - [Jieba Chinse Word Segmenter](https://github.com/fxsjy/jieba)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text into sentences\n",
    "\n",
    "The `sent_tokenize()` function uses an instance of `PunktSentenceTokenizer` from the\n",
    "`nltk.tokenize.punkt` module. This instance has already been trained and works well for many European languages. So it knows what punctuation and characters mark the end of a sentence and the beginning of a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para = \"Python is a widely used general-purpose, high-level programming language. Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java. The language provides constructs intended to enable clear programs on both a small and large scale.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python is a widely used general-purpose, high-level programming language.',\n",
       " 'Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java.',\n",
       " 'The language provides constructs intended to enable clear programs on both a small and large scale.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('Hello World.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'is',\n",
       " 'a',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'general-purpose',\n",
       " ',',\n",
       " 'high-level',\n",
       " 'programming',\n",
       " 'language',\n",
       " '.',\n",
       " 'Its',\n",
       " 'design',\n",
       " 'philosophy',\n",
       " 'emphasizes',\n",
       " 'code',\n",
       " 'readability',\n",
       " ',',\n",
       " 'and',\n",
       " 'its',\n",
       " 'syntax',\n",
       " 'allows',\n",
       " 'programmers',\n",
       " 'to',\n",
       " 'express',\n",
       " 'concepts',\n",
       " 'in',\n",
       " 'fewer',\n",
       " 'lines',\n",
       " 'of',\n",
       " 'code',\n",
       " 'than',\n",
       " 'would',\n",
       " 'be',\n",
       " 'possible',\n",
       " 'in',\n",
       " 'languages',\n",
       " 'such',\n",
       " 'as',\n",
       " 'C++',\n",
       " 'or',\n",
       " 'Java',\n",
       " '.',\n",
       " 'The',\n",
       " 'language',\n",
       " 'provides',\n",
       " 'constructs',\n",
       " 'intended',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'clear',\n",
       " 'programs',\n",
       " 'on',\n",
       " 'both',\n",
       " 'a',\n",
       " 'small',\n",
       " 'and',\n",
       " 'large',\n",
       " 'scale',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing sentences using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering stopwords in a tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all', 'when', 'into', 'now', 'after', 'because', 'of', 'had', 'her', 'ourselves', 'you', 'who', 'hers', 'just', 'being', 'too', 'such', 'very', 'did', 'myself', 'out', 'where', 'most', 'have', 'some', 'an', 'there', 'then', 'himself', 'own', 'if', 'down', 'for', 'were', 'through', 'off', 'few', 'no', 'them', 'he', 'further', 'and', 'nor', 'my', 'itself', 'those', 'does', 'other', 'its', 'not', 'what', 'here', 's', 'i', 'again', 'only', 'with', 'on', 'has', 'having', 'we', 'the', 'doing', 'this', 'why', 'while', 'am', 'whom', 'should', 'themselves', 'can', 'between', 'herself', 'under', 'in', 'any', 'more', 'below', 'a', 'your', 'about', 'yourself', 'was', 'it', 'is', 'or', 'as', 't', 'these', 'both', 'from', 'up', 'once', 'me', 'over', 'same', 'will', 'that', 'during', 'him', 'she', 'so', 'yourselves', 'been', 'to', 'until', 'our', 'do', 'than', 'their', 'before', 'his', 'above', 'by', 'are', 'which', 'be', 'how', 'theirs', 'yours', 'they', 'don', 'each', 'ours', 'at', 'against', 'but'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [\"Can't\", 'is', 'a', 'contraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'contraction']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing and Correcting Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "\n",
    "One of the most common stemming algorithms is the **Porter stemming algorithm** by Martin\n",
    "Porter. It is designed to remove and replace well-known suffixes of English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extream'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('cooking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing repeating characters\n",
    "\n",
    "In everyday language, people are often not strictly grammatical. They will write things such as\n",
    "I looooooove it in order to emphasize the word love . However, computers don't know\n",
    "that \"looooooove\" is a variation of \"love\" unless they are told. This recipe presents a method\n",
    "to remove these annoying repeating characters in order to end up with a proper English word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    \n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in\n",
    "                         patterns]\n",
    "        \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl, s)\n",
    "        return s\n",
    "    \n",
    "class RepeatReplacer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing synonyms\n",
    "\n",
    "It is often useful to reduce the vocabulary of a text by replacing words with common\n",
    "synonyms. By compressing the vocabulary without losing meaning, you can save memory\n",
    "in cases such as frequency analysis and text indexing. Vocabulary reduction\n",
    "can also increase the occurrence of significant collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech Tagging\n",
    "\n",
    "## Training a unigram part-of-speech tagger\n",
    "\n",
    "A unigram generally refers to a single token. Therefore, a unigram tagger only uses a single\n",
    "word as its context for determining the part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger = UnigramTagger(train_sents)\n",
    "treebank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification-based tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "All are statistics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
