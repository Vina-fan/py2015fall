
% Default to the notebook output style




% Inherit from the specified cell style.





\documentclass{article}


\usepackage[adobefonts]{ctex}

    \usepackage{graphicx} % Used to insert images
    \usepackage{adjustbox} % Used to constrain images to a maximum size
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2




    \definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
    \definecolor{darkorange}{rgb}{.71,0.21,0.01}
    \definecolor{darkgreen}{rgb}{.12,.54,.11}
    \definecolor{myteal}{rgb}{.26, .44, .56}
    \definecolor{gray}{gray}{0.45}
    \definecolor{lightgray}{gray}{.95}
    \definecolor{mediumgray}{gray}{.8}
    \definecolor{inputbackground}{rgb}{.95, .95, .85}
    \definecolor{outputbackground}{rgb}{.95, .95, .95}
    \definecolor{traceback}{rgb}{1, .95, .95}
    % ansi colors
    \definecolor{red}{rgb}{.6,0,0}
    \definecolor{green}{rgb}{0,.65,0}
    \definecolor{brown}{rgb}{0.6,0.6,0}
    \definecolor{blue}{rgb}{0,.145,.698}
    \definecolor{purple}{rgb}{.698,.145,.698}
    \definecolor{cyan}{rgb}{0,.698,.698}
    \definecolor{lightgray}{gray}{0.5}

    % bright ansi colors
    \definecolor{darkgray}{gray}{0.25}
    \definecolor{lightred}{rgb}{1.0,0.39,0.28}
    \definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
    \definecolor{lightblue}{rgb}{0.53,0.81,0.92}
    \definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
    \definecolor{lightcyan}{rgb}{0.5,1.0,0.83}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{L5-Python-and-Text-Processing}




    % Pygments definitions

\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}




    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=blue,
      linkcolor=darkorange,
      citecolor=darkgreen,
      }
    % Slightly bigger margins than the latex defaults

    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}



    \begin{document}


    \maketitle




    \section{Concepts in text processing}\label{concepts-in-text-processing}

\subsection{Corpora (语料库)}\label{corpora-ux8bedux6599ux5e93}

Corpus is a large collection of texts. It is a body of written or spoken
material upon which a linguistic analysis is based. A corpus provides
grammarians, lexicographers, and other interested parties with better
discriptions of a language. Computer-procesable corpora allow linguists
to adopt the principle of total accountability, retrieving all the
occurrences of a particular word or structure for inspection or randomly
selcted samples. Corpus analysis provide lexical information,
morphosyntactic information, semantic information and pragmatic
information.

\subsection{Tokens}\label{tokens}

A token is the technical name for a sequence of characters, that we want
to treat as a group. The vocabulary of a text is just the set of tokens
that it uses, since in a set, all duplicates are collapsed together. In
Python we can obtain the vocabulary items with the command:
\texttt{set()}.

\subsection{Stopwords}\label{stopwords}

Stopwords are common words that generally do not contribute to the
meaning of a sentence, at least for the purposes of information
retrieval and natural language processing. These are words such as the
and a. Most search engines will filter out stopwords from search queries
and documents in order to save space in their index.

\subsection{Stemming}\label{stemming}

Stemming is a technique to remove affixes from a word, ending up with
the stem. For example, the stem of cooking is cook , and a good stemming
algorithm knows that the ing suffix can be removed. Stemming is most
commonly used by search engines for indexing words. Instead of storing
all forms of a word, a search engine can store only the stems, greatly
reducing the size of index while increasing retrieval accuracy.

\subsection{Frequency Counts
（频数统计）}\label{frequency-counts-ux9891ux6570ux7edfux8ba1}

Frequency Counts the number of hits. Frequency counts require finding
all the occurences of a particular feature in the corpus. So it is
implicit in concordancing. Software is used for this purpose. Frequency
counts can be explained statistically.

\subsection{Word Segmenter （分词）}\label{word-segmenter-ux5206ux8bcd}

Word segmentation is the problem of dividing a string of written
language into its component words.

In English and many other languages using some form of the Latin
alphabet, the space is a good approximation of a word divider (word
delimiter). (Some examples where the space character alone may not be
sufficient include contractions like can't for can not.)

However the equivalent to this character is not found in all written
scripts, and without it word segmentation is a difficult problem.
Languages which do not have a trivial word segmentation process include
Chinese, Japanese, where sentences but not words are delimited, Thai and
Lao, where phrases and sentences but not words are delimited, and
Vietnamese, where syllables but not words are delimited.

\subsection{Part-Of-Speech Tagger
（词性标注工具）}\label{part-of-speech-tagger-ux8bcdux6027ux6807ux6ce8ux5de5ux5177}

In corpus linguistics, part-of-speech tagging (POS tagging or POST),
also called grammatical tagging or word-category disambiguation, is the
process of marking up a word in a text (corpus) as corresponding to a
particular part of speech, based on both its definition, as well as its
context---i.e.~relationship with adjacent and related words in a phrase,
sentence, or paragraph. A simplified form of this is commonly taught to
school-age children, in the identification of words as nouns, verbs,
adjectives, adverbs, etc.

\subsection{Named Entity
Recognizer（命名实体识别工具）}\label{named-entity-recognizerux547dux540dux5b9eux4f53ux8bc6ux522bux5de5ux5177}

Named-entity recognition (NER) (also known as entity identification,
entity chunking and entity extraction) is a subtask of information
extraction that seeks to locate and classify elements in text into
pre-defined categories such as the names of persons, organizations,
locations, expressions of times, quantities, monetary values,
percentages.

\subsection{Parser（句法分析器）}\label{parserux53e5ux6cd5ux5206ux6790ux5668}

    Here we will treat text as \texttt{raw data} for the programs we write,
programs that manipulate and analyze it in a variety of interesting
ways.

    \section{Natural Language Processing
tools}\label{natural-language-processing-tools}

\begin{itemize}
\item
  Natural Language Toolkit

  \textbf{NLTK} is a leading platform for building Python programs to
  work with human language data. It provides easy-to-use interfaces to
  over 50 corpora and lexical resources such as WordNet, along with a
  suite of text processing libraries for classification, tokenization,
  stemming, tagging, parsing, and semantic reasoning, wrappers for
  industrial-strength NLP libraries, and an active discussion forum.

  Natural Language Processing with Python provides a practical
  introduction to programming for language processing. Written by the
  creators of NLTK, it guides the reader through the fundamentals of
  writing Python programs, working with corpora, categorizing text,
  analyzing linguistic structure, and more. The book is being updated
  for Python 3 and NLTK 3.
\item
  Stanford Word Segmenter

  Tokenization of raw text is a standard pre-processing step for many
  NLP tasks. For English, tokenization usually involves punctuation
  splitting and separation of some affixes like possessives. Other
  languages require more extensive token pre-processing, which is
  usually called segmentation.

  The Stanford Word Segmenter currently supports Arabic and Chinese. The
  provided segmentation schemes have been found to work well for a
  variety of applications.
\item
  NLP toolkits for Chinese

  \begin{itemize}
  \item
    \href{https://github.com/xpqiu/fnlp/}{Toolkit for Chinese natural
    language processing}
  \item
    \href{http://nlp.ict.ac.cn/english/}{The ICT Natural Language
    Processing Research Group}
  \item
    \href{https://github.com/fxsjy/jieba}{Jieba Chinse Word Segmenter}
  \end{itemize}
\end{itemize}

    \section{Tokenizing Text}\label{tokenizing-text}

    \subsection{Tokenizing text into
sentences}\label{tokenizing-text-into-sentences}

The \texttt{sent\_tokenize()} function uses an instance of
\texttt{PunktSentenceTokenizer} from the \texttt{nltk.tokenize.punkt}
module. This instance has already been trained and works well for many
European languages. So it knows what punctuation and characters mark the
end of a sentence and the beginning of a new sentence.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{para} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Python is a widely used general\PYZhy{}purpose, high\PYZhy{}level programming language. Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java. The language provides constructs intended to enable clear programs on both a small and large scale.}\PY{l+s}{\PYZdq{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{tokenize} \PY{k}{import} \PY{n}{sent\PYZus{}tokenize}
        \PY{n}{sent\PYZus{}tokenize}\PY{p}{(}\PY{n}{para}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} ['Python is a widely used general-purpose, high-level programming language.',
         'Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java.',
         'The language provides constructs intended to enable clear programs on both a small and large scale.']
\end{Verbatim}

    \subsection{Tokenizing sentences into
words}\label{tokenizing-sentences-into-words}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{tokenize} \PY{k}{import} \PY{n}{word\PYZus{}tokenize}
         \PY{n}{word\PYZus{}tokenize}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Hello World.}\PY{l+s}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} ['Hello', 'World', '.']
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{word\PYZus{}tokenize}\PY{p}{(}\PY{n}{para}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} ['Python',
          'is',
          'a',
          'widely',
          'used',
          'general-purpose',
          ',',
          'high-level',
          'programming',
          'language',
          '.',
          'Its',
          'design',
          'philosophy',
          'emphasizes',
          'code',
          'readability',
          ',',
          'and',
          'its',
          'syntax',
          'allows',
          'programmers',
          'to',
          'express',
          'concepts',
          'in',
          'fewer',
          'lines',
          'of',
          'code',
          'than',
          'would',
          'be',
          'possible',
          'in',
          'languages',
          'such',
          'as',
          'C++',
          'or',
          'Java',
          '.',
          'The',
          'language',
          'provides',
          'constructs',
          'intended',
          'to',
          'enable',
          'clear',
          'programs',
          'on',
          'both',
          'a',
          'small',
          'and',
          'large',
          'scale',
          '.']
\end{Verbatim}

    \subsection{Tokenizing sentences using regular
expressions}\label{tokenizing-sentences-using-regular-expressions}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{tokenize} \PY{k}{import} \PY{n}{RegexpTokenizer}
         \PY{n}{tokenizer} \PY{o}{=} \PY{n}{RegexpTokenizer}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{[}\PY{l+s}{\PYZbs{}}\PY{l+s}{w}\PY{l+s}{\PYZsq{}}\PY{l+s}{]+}\PY{l+s}{\PYZdq{}}\PY{p}{)}
         \PY{n}{tokenizer}\PY{o}{.}\PY{n}{tokenize}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Can}\PY{l+s}{\PYZsq{}}\PY{l+s}{t is a contraction.}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} ["Can't", 'is', 'a', 'contraction']
\end{Verbatim}

    \subsection{Filtering stopwords in a tokenized
sentence}\label{filtering-stopwords-in-a-tokenized-sentence}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{corpus} \PY{k}{import} \PY{n}{stopwords}
         \PY{n}{english\PYZus{}stops} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{n}{stopwords}\PY{o}{.}\PY{n}{words}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{english}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{english\PYZus{}stops}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'all', 'when', 'into', 'now', 'after', 'because', 'of', 'had', 'her', 'ourselves', 'you', 'who', 'hers', 'just', 'being', 'too', 'such', 'very', 'did', 'myself', 'out', 'where', 'most', 'have', 'some', 'an', 'there', 'then', 'himself', 'own', 'if', 'down', 'for', 'were', 'through', 'off', 'few', 'no', 'them', 'he', 'further', 'and', 'nor', 'my', 'itself', 'those', 'does', 'other', 'its', 'not', 'what', 'here', 's', 'i', 'again', 'only', 'with', 'on', 'has', 'having', 'we', 'the', 'doing', 'this', 'why', 'while', 'am', 'whom', 'should', 'themselves', 'can', 'between', 'herself', 'under', 'in', 'any', 'more', 'below', 'a', 'your', 'about', 'yourself', 'was', 'it', 'is', 'or', 'as', 't', 'these', 'both', 'from', 'up', 'once', 'me', 'over', 'same', 'will', 'that', 'during', 'him', 'she', 'so', 'yourselves', 'been', 'to', 'until', 'our', 'do', 'than', 'their', 'before', 'his', 'above', 'by', 'are', 'which', 'be', 'how', 'theirs', 'yours', 'they', 'don', 'each', 'ours', 'at', 'against', 'but'\}
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{words} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZdq{}}\PY{l+s}{Can}\PY{l+s}{\PYZsq{}}\PY{l+s}{t}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{is}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{a}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{contraction}\PY{l+s}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{p}{[}\PY{n}{word} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{words} \PY{k}{if} \PY{n}{word} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{english\PYZus{}stops}\PY{p}{]}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} ["Can't", 'contraction']
\end{Verbatim}

    \section{Replacing and Correcting
Words}\label{replacing-and-correcting-words}

    \subsection{Stemming}\label{stemming}

One of the most common stemming algorithms is the \textbf{Porter
stemming algorithm} by Martin Porter. It is designed to remove and
replace well-known suffixes of English words

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{stem} \PY{k}{import} \PY{n}{PorterStemmer}
         \PY{n}{stemmer} \PY{o}{=} \PY{n}{PorterStemmer}\PY{p}{(}\PY{p}{)}
         \PY{n}{stemmer}\PY{o}{.}\PY{n}{stem}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{cooking}\PY{l+s}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} 'extream'
\end{Verbatim}

    \subsection{Removing repeating
characters}\label{removing-repeating-characters}

In everyday language, people are often not strictly grammatical. They
will write things such as I looooooove it in order to emphasize the word
love . However, computers don't know that ``looooooove'' is a variation
of ``love'' unless they are told. This recipe presents a method to
remove these annoying repeating characters in order to end up with a
proper English word.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{k+kn}{import} \PY{n+nn}{re}
         \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{corpus} \PY{k}{import} \PY{n}{wordnet}

         \PY{n}{replacement\PYZus{}patterns} \PY{o}{=} \PY{p}{[}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{won}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{t}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{will not}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{can}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{t}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{cannot}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{i}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{m}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{i am}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{ain}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{t}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{is not}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{(}\PY{l+s}{\PYZbs{}}\PY{l+s}{w+)}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{ll}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZbs{}}\PY{l+s}{g\PYZlt{}1\PYZgt{} will}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{(}\PY{l+s}{\PYZbs{}}\PY{l+s}{w+)n}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{t}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZbs{}}\PY{l+s}{g\PYZlt{}1\PYZgt{} not}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{(}\PY{l+s}{\PYZbs{}}\PY{l+s}{w+)}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{ve}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZbs{}}\PY{l+s}{g\PYZlt{}1\PYZgt{} have}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{(}\PY{l+s}{\PYZbs{}}\PY{l+s}{w+)}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{s}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZbs{}}\PY{l+s}{g\PYZlt{}1\PYZgt{} is}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{(}\PY{l+s}{\PYZbs{}}\PY{l+s}{w+)}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{re}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZbs{}}\PY{l+s}{g\PYZlt{}1\PYZgt{} are}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{(}\PY{l+s}{\PYZbs{}}\PY{l+s}{w+)}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s}{d}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZbs{}}\PY{l+s}{g\PYZlt{}1\PYZgt{} would}\PY{l+s}{\PYZsq{}}\PY{p}{)}
         \PY{p}{]}

         \PY{k}{class} \PY{n+nc}{RegexpReplacer}\PY{p}{(}\PY{n+nb}{object}\PY{p}{)}\PY{p}{:}

             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{patterns}\PY{o}{=}\PY{n}{replacement\PYZus{}patterns}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{patterns} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{regex}\PY{p}{)}\PY{p}{,} \PY{n}{repl}\PY{p}{)} \PY{k}{for} \PY{p}{(}\PY{n}{regex}\PY{p}{,} \PY{n}{repl}\PY{p}{)} \PY{o+ow}{in}
                                  \PY{n}{patterns}\PY{p}{]}

             \PY{k}{def} \PY{n+nf}{replace}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{text}\PY{p}{)}\PY{p}{:}
                 \PY{n}{s} \PY{o}{=} \PY{n}{text}
                 \PY{k}{for} \PY{p}{(}\PY{n}{pattern}\PY{p}{,} \PY{n}{repl}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{patterns}\PY{p}{:}
                     \PY{p}{(}\PY{n}{s}\PY{p}{,} \PY{n}{count}\PY{p}{)} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{subn}\PY{p}{(}\PY{n}{pattern}\PY{p}{,} \PY{n}{repl}\PY{p}{,} \PY{n}{s}\PY{p}{)}
                 \PY{k}{return} \PY{n}{s}

         \PY{k}{class} \PY{n+nc}{RepeatReplacer}\PY{p}{(}\PY{n+nb}{object}\PY{p}{)}\PY{p}{:}

             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{repeat\PYZus{}regexp} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{l+s}{r\PYZsq{}}\PY{l+s}{(}\PY{l+s}{\PYZbs{}}\PY{l+s}{w*)(}\PY{l+s}{\PYZbs{}}\PY{l+s}{w)}\PY{l+s}{\PYZbs{}}\PY{l+s}{2(}\PY{l+s}{\PYZbs{}}\PY{l+s}{w*)}\PY{l+s}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{repl} \PY{o}{=} \PY{l+s}{r\PYZsq{}}\PY{l+s}{\PYZbs{}}\PY{l+s}{1}\PY{l+s}{\PYZbs{}}\PY{l+s}{2}\PY{l+s}{\PYZbs{}}\PY{l+s}{3}\PY{l+s}{\PYZsq{}}

             \PY{k}{def} \PY{n+nf}{replace}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{word}\PY{p}{)}\PY{p}{:}
                 \PY{k}{if} \PY{n}{wordnet}\PY{o}{.}\PY{n}{synsets}\PY{p}{(}\PY{n}{word}\PY{p}{)}\PY{p}{:}
                     \PY{k}{return} \PY{n}{word}
                 \PY{n}{repl\PYZus{}word} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{repeat\PYZus{}regexp}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{repl}\PY{p}{,} \PY{n}{word}\PY{p}{)}
                 \PY{k}{if} \PY{n}{repl\PYZus{}word} \PY{o}{!=} \PY{n}{word}\PY{p}{:}
                     \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{repl\PYZus{}word}\PY{p}{)}
                 \PY{k}{else}\PY{p}{:}
                     \PY{k}{return} \PY{n}{repl\PYZus{}word}
\end{Verbatim}

    \subsection{Spelling correction}\label{spelling-correction}

    \subsection{Replacing synonyms}\label{replacing-synonyms}

It is often useful to reduce the vocabulary of a text by replacing words
with common synonyms. By compressing the vocabulary without losing
meaning, you can save memory in cases such as frequency analysis and
text indexing. Vocabulary reduction can also increase the occurrence of
significant collocations

    \section{Part-of-speech Tagging}\label{part-of-speech-tagging}

\subsection{Training a unigram part-of-speech
tagger}\label{training-a-unigram-part-of-speech-tagger}

A unigram generally refers to a single token. Therefore, a unigram
tagger only uses a single word as its context for determining the
part-of-speech tag.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{tag} \PY{k}{import} \PY{n}{UnigramTagger}
         \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{corpus} \PY{k}{import} \PY{n}{treebank}
         \PY{n}{train\PYZus{}sents} \PY{o}{=} \PY{n}{treebank}\PY{o}{.}\PY{n}{tagged\PYZus{}sents}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3000}\PY{p}{]}
         \PY{n}{tagger} \PY{o}{=} \PY{n}{UnigramTagger}\PY{p}{(}\PY{n}{train\PYZus{}sents}\PY{p}{)}
         \PY{n}{treebank}\PY{o}{.}\PY{n}{sents}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}37}]:} ['Pierre',
          'Vinken',
          ',',
          '61',
          'years',
          'old',
          ',',
          'will',
          'join',
          'the',
          'board',
          'as',
          'a',
          'nonexecutive',
          'director',
          'Nov.',
          '29',
          '.']
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n}{tagger}\PY{o}{.}\PY{n}{tag}\PY{p}{(}\PY{n}{treebank}\PY{o}{.}\PY{n}{sents}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}38}]:} [('Pierre', 'NNP'),
          ('Vinken', 'NNP'),
          (',', ','),
          ('61', 'CD'),
          ('years', 'NNS'),
          ('old', 'JJ'),
          (',', ','),
          ('will', 'MD'),
          ('join', 'VB'),
          ('the', 'DT'),
          ('board', 'NN'),
          ('as', 'IN'),
          ('a', 'DT'),
          ('nonexecutive', 'JJ'),
          ('director', 'NN'),
          ('Nov.', 'NNP'),
          ('29', 'CD'),
          ('.', '.')]
\end{Verbatim}

    \subsection{Classification-based
tagging}\label{classification-based-tagging}

    \section{Text Classification}\label{text-classification}

All are statistics!


    % Add a bibliography block to the postdoc



    \end{document}
